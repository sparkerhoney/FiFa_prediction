{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3949d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/lhe339/opt/anaconda3/lib/python3.9/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /Users/lhe339/opt/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6db7904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "\n",
    "# define the URL for each league\n",
    "urls = {\n",
    "    \"EPL\": \"https://understat.com/league/EPL\",\n",
    "    \"La Liga\": \"https://understat.com/league/La_liga\",\n",
    "    \"Serie A\": \"https://understat.com/league/Serie_A\",\n",
    "    \"Ligue 1\": \"https://understat.com/league/Ligue_1\",\n",
    "    \"Bundesliga\": \"https://understat.com/league/Bundesliga\"\n",
    "}\n",
    "\n",
    "# define a function to retrieve player data from a team page\n",
    "def get_player_data(url, team, league):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find_all(\"table\")[0]\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        goals = cols[6].text\n",
    "        assists = cols[7].text\n",
    "        xG = cols[8].text\n",
    "        xA = cols[9].text\n",
    "        player_data.append([league, team, player, goals, assists, xG, xA])\n",
    "    return player_data\n",
    "\n",
    "# create a list to store all player data\n",
    "all_player_data = []\n",
    "\n",
    "# loop over each league\n",
    "for league, url in urls.items():\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    teams = soup.find_all(\"div\", class_=\"progress-table-row\")\n",
    "    # loop over each team in the league\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"div\", class_=\"team-title\").text.strip()\n",
    "        team_url = \"https://understat.com/\" + team.find(\"a\")[\"href\"]\n",
    "        player_data = get_player_data(team_url, team_name, league)\n",
    "        all_player_data += player_data\n",
    "\n",
    "# write the data to a CSV file\n",
    "with open(\"player_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(all_player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ff1f4b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPos\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxG90\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxA90\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxG+xA90\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpxG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpxG+xA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# extract player data\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m tbody \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m rows \u001b[38;5;241m=\u001b[39m tbody\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, \"lxml\")\n",
    "\n",
    "# extract data headers\n",
    "headers = [\"#\", \"Player\", \"Pos\", \"MP\", \"Min\", \"G\", \"A\", \"xG\", \"xA\", \"xG90\", \"xA90\", \"xG+xA90\", \"npxG\", \"npxG+xA\"]\n",
    "\n",
    "# extract player data\n",
    "table = soup.find_all(\"table\")[0]\n",
    "tbody = table.find(\"tbody\")\n",
    "rows = tbody.find_all(\"tr\")\n",
    "\n",
    "players = []\n",
    "for row in rows:\n",
    "    player_data = []\n",
    "    for td in row.find_all(\"td\"):\n",
    "        player_data.append(td.text.strip())\n",
    "    player = dict(zip(headers, player_data))\n",
    "    players.append(player)\n",
    "\n",
    "print(players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4246fa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# extract player data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m tbody \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m rows \u001b[38;5;241m=\u001b[39m tbody\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://fbref.com/en/squads/4b78f8bb/Arsenal-Stats\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, \"lxml\")\n",
    "\n",
    "# extract player data\n",
    "table = soup.find_all(\"table\")[0]\n",
    "tbody = table.find(\"tbody\")\n",
    "rows = tbody.find_all(\"tr\")\n",
    "\n",
    "player_data = []\n",
    "for row in rows:\n",
    "    data = {}\n",
    "    cols = row.find_all(\"td\")\n",
    "    data[\"number\"] = cols[0].text.strip()\n",
    "    data[\"name\"] = cols[1].text.strip()\n",
    "    data[\"position\"] = cols[2].text.strip()\n",
    "    data[\"matches_played\"] = cols[3].text.strip()\n",
    "    data[\"minutes_played\"] = cols[4].text.strip()\n",
    "    data[\"goals\"] = cols[5].text.strip()\n",
    "    data[\"assists\"] = cols[6].text.strip()\n",
    "    data[\"xG\"] = cols[7].text.strip()\n",
    "    data[\"xA\"] = cols[8].text.strip()\n",
    "    data[\"xG90\"] = cols[9].text.strip()\n",
    "    data[\"xA90\"] = cols[10].text.strip()\n",
    "    data[\"xG+xA90\"] = cols[11].text.strip()\n",
    "    data[\"npxG\"] = cols[12].text.strip()\n",
    "    data[\"npxG+xA\"] = cols[13].text.strip()\n",
    "    player_data.append(data)\n",
    "\n",
    "print(player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bb6df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "\n",
    "# define the URL for each league\n",
    "urls = {\n",
    "    \"EPL\": \"https://understat.com/league/EPL\",\n",
    "    \"La Liga\": \"https://understat.com/league/La_liga\",\n",
    "    \"Serie A\": \"https://understat.com/league/Serie_A\",\n",
    "    \"Ligue 1\": \"https://understat.com/league/Ligue_1\",\n",
    "    \"Bundesliga\": \"https://understat.com/league/Bundesliga\"\n",
    "}\n",
    "\n",
    "# define a function to retrieve player data from a team page\n",
    "def get_player_data(url, team, league):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find_all(\"table\")[0]\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        goals = cols[6].text\n",
    "        assists = cols[7].text\n",
    "        xG = cols[8].text\n",
    "        xA = cols[9].text\n",
    "        player_data.append([league, team, player, goals, assists, xG, xA])\n",
    "    return player_data\n",
    "\n",
    "# create a list to store all player data\n",
    "all_player_data = []\n",
    "\n",
    "# loop over each league\n",
    "for league, url in urls.items():\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    teams = soup.find_all(\"div\", class_=\"progress-table-row\")\n",
    "    # loop over each team in the league\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"div\", class_=\"team-title\").text.strip()\n",
    "        team_url = \"https://understat.com/\" + team.find(\"a\")[\"href\"]\n",
    "        player_data = get_player_data(team_url, team_name, league)\n",
    "        all_player_data += player_data\n",
    "\n",
    "# write the data to a CSV file\n",
    "with open(\"player_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(all_player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53d16859",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m league \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m team \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArsenal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m player_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_player_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleague\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# write the data to a CSV file\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marsenal_player_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mget_player_data\u001b[0;34m(url, team, league)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_player_data\u001b[39m(url, team, league):\n\u001b[1;32m     10\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(requests\u001b[38;5;241m.\u001b[39mget(url)\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m     tbody \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     rows \u001b[38;5;241m=\u001b[39m tbody\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "\n",
    "# define a function to retrieve player data from a team page\n",
    "def get_player_data(url, team, league):\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"lxml\")\n",
    "    table = soup.find_all(\"table\")[0]\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        goals = cols[6].text\n",
    "        assists = cols[7].text\n",
    "        xG = cols[10].text\n",
    "        xA = cols[11].text\n",
    "        player_data.append([league, team, player, goals, assists, xG, xA])\n",
    "    return player_data\n",
    "\n",
    "# get player data for Arsenal in the 2018 season\n",
    "team_url = \"https://understat.com/team/Arsenal/2018\"\n",
    "league = \"EPL\"\n",
    "team = \"Arsenal\"\n",
    "player_data = get_player_data(team_url, team, league)\n",
    "\n",
    "# write the data to a CSV file\n",
    "with open(\"arsenal_player_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e65aa321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m league \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m team \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArsenal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m player_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_player_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleague\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# write the data to a CSV file\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marsenal_player_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mget_player_data\u001b[0;34m(url, team, league)\u001b[0m\n\u001b[1;32m     14\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats-table\u001b[39m\u001b[38;5;124m\"\u001b[39m})  \u001b[38;5;66;03m# find the table with player stats\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m tbody \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m rows \u001b[38;5;241m=\u001b[39m tbody\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m player_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Position\", \"Matches\", \"Minutes\", \"Goals\", \"Assists\", \"xG\", \"xA\", \"xG90\", \"xA90\"]\n",
    "\n",
    "# define the URL for the team page\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "\n",
    "# define a function to retrieve player data from a team page\n",
    "def get_player_data(url, team, league):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find(\"table\", {\"class\": \"stats-table\"})  # find the table with player stats\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        position = cols[2].text\n",
    "        matches = cols[3].text\n",
    "        minutes = cols[4].text\n",
    "        goals = cols[5].text\n",
    "        assists = cols[6].text\n",
    "        xG = cols[7].text\n",
    "        xA = cols[8].text\n",
    "        xG90 = cols[9].text\n",
    "        xA90 = cols[10].text\n",
    "        player_data.append([league, team, player, position, matches, minutes, goals, assists, xG, xA, xG90, xA90])\n",
    "    return player_data\n",
    "\n",
    "# get the player data for Arsenal in the 2018 Premier League\n",
    "league = \"EPL\"\n",
    "team = \"Arsenal\"\n",
    "player_data = get_player_data(url, team, league)\n",
    "\n",
    "# write the data to a CSV file\n",
    "with open(\"arsenal_player_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "491231e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m player_data\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# get the player data for the 2018/2019 season\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m player_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_player_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# write the data to a CSV file\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mget_player_data\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     14\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m---> 16\u001b[0m tbody \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m rows \u001b[38;5;241m=\u001b[39m tbody\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m player_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"Player\", \"Position\", \"Squad\", \"Comp\", \"Age\", \"Min\", \"90s\", \"Gls\", \"Ast\", \"G-PK\", \"PK\", \"PKatt\", \"CrdY\", \"CrdR\", \"xG\", \"npxG\", \"xA\", \"npxG+xA\", \"xG90\", \"xA90\", \"xG+xA90\", \"npxG90\", \"npxG+xA90\"]\n",
    "\n",
    "# define the URL for the player stats page\n",
    "url = \"https://fbref.com/en/comps/Big5/2018-2019/stats/players/2018-2019-Big-5-European-Leagues-Stats\"\n",
    "\n",
    "# define a function to retrieve player data from the stats page\n",
    "def get_player_data(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find(\"table\", {\"id\": \"stats\"})\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[0].text\n",
    "        position = cols[1].text\n",
    "        squad = cols[2].text\n",
    "        comp = cols[3].text\n",
    "        age = cols[4].text\n",
    "        minutes = cols[5].text\n",
    "        nineties = cols[6].text\n",
    "        goals = cols[7].text\n",
    "        assists = cols[8].text\n",
    "        goals_pk = cols[9].text\n",
    "        pk = cols[10].text\n",
    "        pk_att = cols[11].text\n",
    "        yellow = cols[12].text\n",
    "        red = cols[13].text\n",
    "        xg = cols[14].text\n",
    "        npxg = cols[15].text\n",
    "        xa = cols[16].text\n",
    "        npxg_xa = cols[17].text\n",
    "        xg90 = cols[18].text\n",
    "        xa90 = cols[19].text\n",
    "        xg_xa90 = cols[20].text\n",
    "        npxg90 = cols[21].text\n",
    "        npxg_xa90 = cols[22].text\n",
    "        player_data.append([player, position, squad, comp, age, minutes, nineties, goals, assists, goals_pk, pk, pk_att, yellow, red, xg, npxg, xa, npxg_xa, xg90, xa90, xg_xa90, npxg90, npxg_xa90])\n",
    "    return player_data\n",
    "\n",
    "# get the player data for the 2018/2019 season\n",
    "player_data = get_player_data(url)\n",
    "\n",
    "# write the data to a CSV file\n",
    "with open(\"player_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b9f71d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m stats_table \u001b[38;5;241m=\u001b[39m player_soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     17\u001b[0m row \u001b[38;5;241m=\u001b[39m stats_table\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-row\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. Access the page\n",
    "url = 'https://fbref.com/en/comps/Big5/2018-2019/stats/players/2018-2019-Big-5-European-Leagues-Stats'\n",
    "response = requests.get(url)\n",
    "\n",
    "# 2. Locate Ismael Aaneba's page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "player_link = soup.find('a', text='Ismael Aaneba')['href']\n",
    "\n",
    "# 3. Bring the player's performance and competence for the 2018/2019 season\n",
    "player_url = 'https://fbref.com' + player_link\n",
    "player_response = requests.get(player_url)\n",
    "player_soup = BeautifulSoup(player_response.content, 'html.parser')\n",
    "stats_table = player_soup.find('tr', {'id': 'stats'})\n",
    "row = stats_table.find('tr', {'data-row': '1'})\n",
    "print(row.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89b17782",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(reader)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print the second row of the data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# read the CSV file\n",
    "with open(\"player_data.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "# print the second row of the data\n",
    "print(data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663d30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eedca83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "\n",
    "# define the URL for each league and season\n",
    "urls = {\n",
    "    \"EPL\": \"https://understat.com/league/EPL/2018\",\n",
    "    \"La Liga\": \"https://understat.com/league/La_liga/2018\",\n",
    "    \"Serie A\": \"https://understat.com/league/Serie_A/2018\",\n",
    "    \"Ligue 1\": \"https://understat.com/league/Ligue_1/2018\",\n",
    "    \"Bundesliga\": \"https://understat.com/league/Bundesliga/2018\"\n",
    "}\n",
    "\n",
    "# define a function to retrieve player data from a team page\n",
    "def get_player_data(url, team, league):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find_all(\"table\")[0]\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        goals = cols[6].text\n",
    "        assists = cols[7].text\n",
    "        xG = cols[10].text\n",
    "        xA = cols[11].text\n",
    "        player_data.append([league, team, player, goals, assists, xG, xA])\n",
    "    return player_data\n",
    "\n",
    "# create a list to store all player data\n",
    "all_player_data = []\n",
    "\n",
    "# loop over each league\n",
    "for league, url in urls.items():\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    teams = soup.find_all(\"div\", class_=\"progress-table-row\")\n",
    "    # loop over each team in the league\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"div\", class_=\"team-title\").text.strip()\n",
    "        team_url = \"https://understat.com/\" + team.find(\"a\")[\"href\"]\n",
    "        print(f\"Getting data for {team_name} in {league}...\")\n",
    "        player_data = get_player_data(team_url, team_name, league)\n",
    "        print(f\"Retrieved data for {len(player_data)} players.\")\n",
    "        all_player_data += player_data\n",
    "\n",
    "# write the data to a CSV file\n",
    "with open(\"player_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(all_player_data)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "091e3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "\n",
    "# define the URL for each league and season\n",
    "urls = {\n",
    "    \"EPL\": \"https://understat.com/league/EPL/2018\",\n",
    "    \"La Liga\": \"https://understat.com/league/La_liga/2018\",\n",
    "    \"Serie A\": \"https://understat.com/league/Serie_A/2018\",\n",
    "    \"Ligue 1\": \"https://understat.com/league/Ligue_1/2018\",\n",
    "    \"Bundesliga\": \"https://understat.com/league/Bundesliga/2018\"\n",
    "}\n",
    "\n",
    "# define a function to retrieve player data from a team page\n",
    "def get_player_data(url, team, league):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find_all(\"table\")[0]\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        goals = cols[6].text\n",
    "        assists = cols[7].text\n",
    "        xG = cols[10].text\n",
    "        xA = cols[11].text\n",
    "        player_data.append([league, team, player, goals, assists, xG, xA])\n",
    "    return player_data\n",
    "\n",
    "# create a list to store all player data\n",
    "all_player_data = []\n",
    "\n",
    "# loop over each league\n",
    "for league, url in urls.items():\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    teams = soup.find_all(\"div\", class_=\"progress-table-row\")\n",
    "    # loop over each team in the league\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"div\", class_=\"team-title\").text.strip()\n",
    "        team_url = \"https://understat.com/\" + team.find(\"a\")[\"href\"]\n",
    "        player_data = get_player_data(team_url, team_name, league)\n",
    "        all_player_data += player_data\n",
    "\n",
    "# write the data to a CSV file\n",
    "with open(\"player_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(all_player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d4eba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 rows to player_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "\n",
    "# define the URL for each league and season\n",
    "urls = {\n",
    "    \"EPL\": \"https://understat.com/league/EPL/2018\",\n",
    "    \"La Liga\": \"https://understat.com/league/La_liga/2018\",\n",
    "    \"Serie A\": \"https://understat.com/league/Serie_A/2018\",\n",
    "    \"Ligue 1\": \"https://understat.com/league/Ligue_1/2018\",\n",
    "    \"Bundesliga\": \"https://understat.com/league/Bundesliga/2018\"\n",
    "}\n",
    "\n",
    "# define a function to retrieve player data from a team page\n",
    "def get_player_data(url, team, league):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find_all(\"table\")[0]\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        goals = cols[6].text\n",
    "        assists = cols[7].text\n",
    "        xG = cols[10].text\n",
    "        xA = cols[11].text\n",
    "        player_data.append([league, team, player, goals, assists, xG, xA])\n",
    "    return player_data\n",
    "\n",
    "# create a list to store all player data\n",
    "all_player_data = []\n",
    "\n",
    "# loop over each league\n",
    "for league, url in urls.items():\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    teams = soup.find_all(\"div\", class_=\"progress-table-row\")\n",
    "    # loop over each team in the league\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"div\", class_=\"team-title\").text.strip()\n",
    "        team_url = \"https://understat.com/\" + team.find(\"a\")[\"href\"]\n",
    "        player_data = get_player_data(team_url, team_name, league)\n",
    "        all_player_data += player_data\n",
    "        print(f\"Retrieved {len(player_data)} player data from {team_name} in {league}\")\n",
    "\n",
    "# write the data to a CSV file\n",
    "with open(\"player_data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(all_player_data)\n",
    "    print(f\"Wrote {len(all_player_data)} rows to player_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e6f18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "\n",
    "# define the URL for each league and season\n",
    "urls = {\n",
    "    \"EPL\": \"https://understat.com/league/EPL/2018\",\n",
    "    \"La Liga\": \"https://understat.com/league/La_liga/2018\",\n",
    "    \"Serie A\": \"https://understat.com/league/Serie_A/2018\",\n",
    "    \"Ligue 1\": \"https://understat.com/league/Ligue_1/2018\",\n",
    "    \"Bundesliga\": \"https://understat.com/league/Bundesliga/2018\"\n",
    "}\n",
    "\n",
    "# define a function to retrieve player data from a team page\n",
    "def get_player_data(url, team, league):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find_all(\"table\")[0]\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        goals = cols[6].text\n",
    "        assists = cols[7].text\n",
    "        xG = cols[10].text\n",
    "        xA = cols[11].text\n",
    "        player_data.append({\n",
    "            \"League\": league,\n",
    "            \"Team\": team,\n",
    "            \"Player\": player,\n",
    "            \"Goals\": goals,\n",
    "            \"Assists\": assists,\n",
    "            \"xG\": xG,\n",
    "            \"xA\": xA\n",
    "        })\n",
    "    return player_data\n",
    "\n",
    "# create a list to store all player data\n",
    "all_player_data = []\n",
    "\n",
    "# loop over each league\n",
    "for league, url in urls.items():\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    teams = soup.find_all(\"div\", class_=\"progress-table-row\")\n",
    "    # loop over each team in the league\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"div\", class_=\"team-title\").text.strip()\n",
    "        team_url = \"https://understat.com/\" + team.find(\"a\")[\"href\"]\n",
    "        player_data = get_player_data(team_url, team_name, league)\n",
    "        all_player_data += player_data\n",
    "\n",
    "# print the data as a list of dictionaries\n",
    "for player in all_player_data:\n",
    "    print(player)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57ea172c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayers\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract the table headers\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m headers \u001b[38;5;241m=\u001b[39m [th\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthead th\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Extract the table rows\u001b[39;00m\n\u001b[1;32m     19\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "res = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the data we want\n",
    "table = soup.find(\"table\", {\"id\": \"players\"})\n",
    "\n",
    "# Extract the table headers\n",
    "headers = [th.text.strip() for th in table.select(\"thead th\")]\n",
    "\n",
    "# Extract the table rows\n",
    "data = []\n",
    "for tr in table.select(\"tbody tr\"):\n",
    "    # Extract the data for each row\n",
    "    row = [td.text.strip() for td in tr.select(\"td\")]\n",
    "    # Add the row to the data list\n",
    "    data.append(row)\n",
    "\n",
    "# Create a list of dictionaries, where each dictionary represents a player and their statistics\n",
    "player_stats = []\n",
    "for row in data:\n",
    "    player_stats.append({\n",
    "        headers[0]: row[0],    # League\n",
    "        headers[1]: row[1],    # Team\n",
    "        headers[2]: row[2],    # Player\n",
    "        headers[3]: row[3],    # Goals\n",
    "        headers[4]: row[4],    # Assists\n",
    "        headers[5]: row[5],    # xG\n",
    "        headers[6]: row[6],    # xA\n",
    "    })\n",
    "\n",
    "# Print the list of player statistics\n",
    "print(player_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38247d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "res = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the data we want\n",
    "table = soup.find(\"table\", {\"id\": \"players\"})\n",
    "\n",
    "if table is not None:\n",
    "    # Extract the table headers\n",
    "    headers = [th.text.strip() for th in table.select(\"thead th\")]\n",
    "\n",
    "    # Extract the table rows\n",
    "    data = []\n",
    "    for tr in table.select(\"tbody tr\"):\n",
    "        # Extract the data for each row\n",
    "        row = [td.text.strip() for td in tr.select(\"td\")]\n",
    "        # Add the row to the data list\n",
    "        data.append(row)\n",
    "\n",
    "    # Create a list of dictionaries, where each dictionary represents a player and their statistics\n",
    "    player_stats = []\n",
    "    for row in data:\n",
    "        player_stats.append({\n",
    "            headers[0]: row[0],    # League\n",
    "            headers[1]: row[1],    # Team\n",
    "            headers[2]: row[2],    # Player\n",
    "            headers[3]: row[3],    # Goals\n",
    "            headers[4]: row[4],    # Assists\n",
    "            headers[5]: row[5],    # xG\n",
    "            headers[6]: row[6],    # xA\n",
    "        })\n",
    "\n",
    "    # Print the list of player statistics\n",
    "    print(player_stats)\n",
    "else:\n",
    "    print(\"Table not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37a99a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "res = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "# Find all tables in the page\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "# Print the IDs of all tables in the page\n",
    "for table in tables:\n",
    "    print(table.get(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d843010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Find the table rows\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstats-table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract the data from each row\u001b[39;00m\n\u001b[1;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "res = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "# Find the table rows\n",
    "rows = soup.find(\"table\", {\"class\": \"stats-table\"}).find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "# Extract the data from each row\n",
    "data = []\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    player = cells[1].text.strip()\n",
    "    position = cells[2].text.strip()\n",
    "    goals = cells[5].text.strip()\n",
    "    assists = cells[6].text.strip()\n",
    "    xG = cells[7].text.strip()\n",
    "    xA = cells[8].text.strip()\n",
    "    team = \"Arsenal\"\n",
    "    league = \"EPL\"\n",
    "    data.append({\n",
    "        \"Player\": player,\n",
    "        \"Position\": position,\n",
    "        \"Goals\": goals,\n",
    "        \"Assists\": assists,\n",
    "        \"xG\": xG,\n",
    "        \"xA\": xA,\n",
    "        \"Team\": team,\n",
    "        \"League\": league\n",
    "    })\n",
    "\n",
    "# Print the extracted data\n",
    "for d in data:\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63c8d0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m stat_table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract the table headers\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m headers \u001b[38;5;241m=\u001b[39m [th\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstat_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthead th\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract the table rows\u001b[39;00m\n\u001b[1;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "res = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "# Find the stat table\n",
    "stat_table = soup.find(\"table\")\n",
    "\n",
    "# Extract the table headers\n",
    "headers = [th.get_text(strip=True) for th in stat_table.select(\"thead th\")]\n",
    "\n",
    "# Extract the table rows\n",
    "data = []\n",
    "for row in stat_table.select(\"tbody tr\"):\n",
    "    values = [td.get_text(strip=True) for td in row.select(\"td\")]\n",
    "    data.append(dict(zip(headers, values)))\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74833903",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m stat_table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats-table\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract the table headers\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m headers \u001b[38;5;241m=\u001b[39m [th\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstat_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthead th\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract the table rows\u001b[39;00m\n\u001b[1;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://understat.com/league/EPL\"\n",
    "res = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "# Find the stat table\n",
    "stat_table = soup.find(\"table\", {\"class\": \"stats-table\"})\n",
    "\n",
    "# Extract the table headers\n",
    "headers = [th.get_text(strip=True) for th in stat_table.select(\"thead th\")]\n",
    "\n",
    "# Extract the table rows\n",
    "data = []\n",
    "for row in stat_table.select(\"tbody tr\"):\n",
    "    row_data = [td.get_text(strip=True) for td in row.find_all(\"td\")]\n",
    "    data.append(row_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7f05f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# define the headers of the CSV file\n",
    "headers = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "\n",
    "# define the URL for each league and season\n",
    "urls = {\n",
    "    \"EPL\": \"https://understat.com/league/EPL/2018\",\n",
    "    \"La Liga\": \"https://understat.com/league/La_liga/2018\",\n",
    "    \"Serie A\": \"https://understat.com/league/Serie_A/2018\",\n",
    "    \"Ligue 1\": \"https://understat.com/league/Ligue_1/2018\",\n",
    "    \"Bundesliga\": \"https://understat.com/league/Bundesliga/2018\"\n",
    "}\n",
    "\n",
    "# define a function to retrieve player data from a team page and return as a dictionary\n",
    "def get_player_data_dict(url, team, league):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find_all(\"table\")[0]\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    player_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        player = cols[1].find(\"a\").text\n",
    "        goals = cols[6].text\n",
    "        assists = cols[7].text\n",
    "        xG = cols[10].text\n",
    "        xA = cols[11].text\n",
    "        player_dict = {\n",
    "            \"League\": league,\n",
    "            \"Team\": team,\n",
    "            \"Player\": player,\n",
    "            \"Goals\": goals,\n",
    "            \"Assists\": assists,\n",
    "            \"xG\": xG,\n",
    "            \"xA\": xA\n",
    "        }\n",
    "        player_data.append(player_dict)\n",
    "    return player_data\n",
    "\n",
    "# create a list to store all player data\n",
    "all_player_data = []\n",
    "\n",
    "# loop over each league\n",
    "for league, url in urls.items():\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    teams = soup.find_all(\"div\", class_=\"progress-table-row\")\n",
    "    # loop over each team in the league\n",
    "    for team in teams:\n",
    "        team_name = team.find(\"div\", class_=\"team-title\").text.strip()\n",
    "        team_url = \"https://understat.com/\" + team.find(\"a\")[\"href\"]\n",
    "        player_data = get_player_data_dict(team_url, team_name, league)\n",
    "        all_player_data += player_data\n",
    "\n",
    "# print the data as dictionaries\n",
    "for player in all_player_data:\n",
    "    print(player)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ffa1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class PlayerDataSpider(scrapy.Spider):\n",
    "    name = \"player_data\"\n",
    "    \n",
    "    start_urls = [\n",
    "        \"https://understat.com/league/EPL/2018\",\n",
    "        \"https://understat.com/league/La_liga/2018\",\n",
    "        \"https://understat.com/league/Serie_A/2018\",\n",
    "        \"https://understat.com/league/Ligue_1/2018\",\n",
    "        \"https://understat.com/league/Bundesliga/2018\"\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        league = response.url.split(\"/\")[-2].replace(\"_\", \" \").upper()\n",
    "        for team in response.css(\"div.progress-table-row\"):\n",
    "            team_name = team.css(\"div.team-title::text\").get().strip()\n",
    "            team_url = response.urljoin(team.css(\"a::attr(href)\").get())\n",
    "            yield scrapy.Request(team_url, meta={\"team\": team_name, \"league\": league}, callback=self.parse_team)\n",
    "    \n",
    "    def parse_team(self, response):\n",
    "        team = response.request.meta[\"team\"]\n",
    "        league = response.request.meta[\"league\"]\n",
    "        for row in response.css(\"table tbody tr\"):\n",
    "            player = row.css(\"td:nth-child(2) a::text\").get()\n",
    "            goals = row.css(\"td:nth-child(7)::text\").get()\n",
    "            assists = row.css(\"td:nth-child(8)::text\").get()\n",
    "            xG = row.css(\"td:nth-child(11)::text\").get()\n",
    "            xA = row.css(\"td:nth-child(12)::text\").get()\n",
    "            yield {\n",
    "                \"League\": league,\n",
    "                \"Team\": team,\n",
    "                \"Player\": player,\n",
    "                \"Goals\": goals,\n",
    "                \"Assists\": assists,\n",
    "                \"xG\": xG,\n",
    "                \"xA\": xA\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51c2c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import csv\n",
    "\n",
    "class PlayerDataSpider(scrapy.Spider):\n",
    "    name = \"player_data\"\n",
    "    \n",
    "    start_urls = [\n",
    "        \"https://understat.com/league/EPL/2018\",\n",
    "        \"https://understat.com/league/La_liga/2018\",\n",
    "        \"https://understat.com/league/Serie_A/2018\",\n",
    "        \"https://understat.com/league/Ligue_1/2018\",\n",
    "        \"https://understat.com/league/Bundesliga/2018\"\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        league = response.url.split(\"/\")[-2].replace(\"_\", \" \").upper()\n",
    "        for team in response.css(\"div.progress-table-row\"):\n",
    "            team_name = team.css(\"div.team-title::text\").get().strip()\n",
    "            team_url = response.urljoin(team.css(\"a::attr(href)\").get())\n",
    "            yield scrapy.Request(team_url, meta={\"team\": team_name, \"league\": league}, callback=self.parse_team)\n",
    "    \n",
    "    def parse_team(self, response):\n",
    "        team = response.request.meta[\"team\"]\n",
    "        league = response.request.meta[\"league\"]\n",
    "        for row in response.css(\"table tbody tr\"):\n",
    "            player = row.css(\"td:nth-child(2) a::text\").get()\n",
    "            goals = row.css(\"td:nth-child(7)::text\").get()\n",
    "            assists = row.css(\"td:nth-child(8)::text\").get()\n",
    "            xG = row.css(\"td:nth-child(11)::text\").get()\n",
    "            xA = row.css(\"td:nth-child(12)::text\").get()\n",
    "            yield {\n",
    "                \"League\": league,\n",
    "                \"Team\": team,\n",
    "                \"Player\": player,\n",
    "                \"Goals\": goals,\n",
    "                \"Assists\": assists,\n",
    "                \"xG\": xG,\n",
    "                \"xA\": xA\n",
    "            }\n",
    "\n",
    "            # write the data to a CSV file\n",
    "            with open('player_data.csv', mode='a', newline='') as csv_file:\n",
    "                fieldnames = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    \"League\": league,\n",
    "                    \"Team\": team,\n",
    "                    \"Player\": player,\n",
    "                    \"Goals\": goals,\n",
    "                    \"Assists\": assists,\n",
    "                    \"xG\": xG,\n",
    "                    \"xA\": xA\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5bbc7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import csv\n",
    "\n",
    "class PlayerDataSpider(scrapy.Spider):\n",
    "    name = \"player_data\"\n",
    "    \n",
    "    start_urls = [\n",
    "        \"https://understat.com/league/EPL/2018\",\n",
    "        \"https://understat.com/league/La_liga/2018\",\n",
    "        \"https://understat.com/league/Serie_A/2018\",\n",
    "        \"https://understat.com/league/Ligue_1/2018\",\n",
    "        \"https://understat.com/league/Bundesliga/2018\"\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        league = response.url.split(\"/\")[-2].replace(\"_\", \" \").upper()\n",
    "        for team in response.css(\"div.progress-table-row\"):\n",
    "            team_name = team.css(\"div.team-title::text\").get().strip()\n",
    "            team_url = response.urljoin(team.css(\"a::attr(href)\").get())\n",
    "            yield scrapy.Request(team_url, meta={\"team\": team_name, \"league\": league}, callback=self.parse_team)\n",
    "    \n",
    "    def parse_team(self, response):\n",
    "        team = response.request.meta[\"team\"]\n",
    "        league = response.request.meta[\"league\"]\n",
    "        for row in response.css(\"table tbody tr\"):\n",
    "            player = row.css(\"td:nth-child(2) a::text\").get()\n",
    "            goals = row.css(\"td:nth-child(7)::text\").get()\n",
    "            assists = row.css(\"td:nth-child(8)::text\").get()\n",
    "            xG = row.css(\"td:nth-child(11)::text\").get()\n",
    "            xA = row.css(\"td:nth-child(12)::text\").get()\n",
    "            item = {\n",
    "                \"League\": league,\n",
    "                \"Team\": team,\n",
    "                \"Player\": player,\n",
    "                \"Goals\": goals,\n",
    "                \"Assists\": assists,\n",
    "                \"xG\": xG,\n",
    "                \"xA\": xA\n",
    "            }\n",
    "            yield item\n",
    "\n",
    "            # write the data to a CSV file\n",
    "            with open('player_data.csv', mode='a', newline='') as csv_file:\n",
    "                fieldnames = [\"League\", \"Team\", \"Player\", \"Goals\", \"Assists\", \"xG\", \"xA\"]\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                writer.writerow(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e72ff2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Arsenal   URL\n",
    "url = 'https://understat.com/team/Arsenal/2018'\n",
    "\n",
    "# HTTP    \n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# BeautifulSoup  \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#  \n",
    "links = []\n",
    "for a in soup.find_all('a', href=True):\n",
    "    if '/player/' in a['href']:\n",
    "        links.append('https://understat.com' + a['href'])\n",
    "\n",
    "#        \n",
    "for link in links:\n",
    "    response = requests.get(link)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    name = soup.find('div', {'class': 'card-header'}).text\n",
    "    print(name)\n",
    "    #     \n",
    "    # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4c69dee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'tbody'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam-data\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# loop through each row in the table and extract the relevant stats\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtbody\u001b[49m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     18\u001b[0m     player_name \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer_name\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     19\u001b[0m     goals \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-stat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoals\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tbody'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set up the URL and headers\n",
    "url = 'https://understat.com/league/EPL/2018'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# send the request and get the page content\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the table containing the player stats\n",
    "table = soup.find('table', {'id': 'team-data'})\n",
    "\n",
    "# loop through each row in the table and extract the relevant stats\n",
    "for row in table.tbody.find_all('tr'):\n",
    "    player_name = row.find('a', {'class': 'player_name'}).text\n",
    "    goals = row.find('td', {'data-stat': 'goals'}).text\n",
    "    assists = row.find('td', {'data-stat': 'assists'}).text\n",
    "    xG = row.find('td', {'data-stat': 'xG'}).text\n",
    "    xA = row.find('td', {'data-stat': 'xA'}).text\n",
    "    print(f\"Player: {player_name}, Goals: {goals}, Assists: {assists}, xG: {xG}, xA: {xA}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "963f977b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstats_10712_2018\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# extract the relevant stats from the table\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m goals \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoals\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     19\u001b[0m assists \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massists\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     20\u001b[0m minutes_played \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-stat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminutes_90s\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# specify the url for the player page\n",
    "url = 'https://fbref.com/en/players/a7e4760c/Ismael-Aaneba'\n",
    "\n",
    "# send a GET request to the page and get the content\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "\n",
    "# parse the content using BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# find the table containing the player stats for the 2018/2019 season\n",
    "table = soup.find('table', {'id': 'stats_10712_2018'})\n",
    "\n",
    "# extract the relevant stats from the table\n",
    "goals = table.find('td', {'data-stat': 'goals'}).text\n",
    "assists = table.find('td', {'data-stat': 'assists'}).text\n",
    "minutes_played = table.find('td', {'data-stat': 'minutes_90s'}).text\n",
    "\n",
    "# print the stats\n",
    "print(f\"Goals: {goals}, Assists: {assists}, Minutes played: {minutes_played}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "947a1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5423d24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requests.get() url \n",
    "headers = {'User-Agent' : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\"}\n",
    "\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1028a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser') # r.content  r.text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b94b6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info= soup.find_all('tr', class_=['odd', 'even'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85b0998a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mplayer_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#   \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(player_info))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(player_info[0])\n",
    "\n",
    "#   \n",
    "print(len(player_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f3b81a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find player with name Pierre-Emerick Aubameyang\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://understat.com/team/Arsenal/2018\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "#      tr  .\n",
    "player_info = soup.find_all('tr', class_=['odd', 'even'])\n",
    "\n",
    "for player in player_info:\n",
    "    #   <a>   ,    .\n",
    "    player_name = player.find('a').text\n",
    "\n",
    "    #    <td>   ,    .\n",
    "    height = player.find_all('td')[3].text\n",
    "    weight = player.find_all('td')[4].text\n",
    "\n",
    "    #   .\n",
    "    print(\"Player name:\", player_name)\n",
    "    print(\"Height:\", height)\n",
    "    print(\"Weight:\", weight)\n",
    "    print(\"----------------------\")\n",
    "\n",
    "\n",
    "player_name = \"Pierre-Emerick Aubameyang\"\n",
    "player_id = None\n",
    "player_stats = None\n",
    "\n",
    "player_rows = soup.find_all('tr', class_=['odd', 'even'])\n",
    "\n",
    "for row in player_rows:\n",
    "    row_name = row.find('td', class_='player-title').text.strip()\n",
    "    if row_name == player_name:\n",
    "        player_id = row.find('td', class_='player-title').find('a')['href'].split('/')[-1]\n",
    "        player_stats = [stat.text.strip() for stat in row.find_all('td')][3:]\n",
    "\n",
    "if player_id is not None and player_stats is not None:\n",
    "    print(f\"Player name: {player_name}\")\n",
    "    print(f\"Player ID: {player_id}\")\n",
    "    print(f\"Stats: {player_stats}\")\n",
    "else:\n",
    "    print(f\"Could not find player with name {player_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca769fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     14\u001b[0m wait \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# parse the HTML content using BeautifulSoup\u001b[39;00m\n\u001b[1;32m     18\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/support/wait.py:80\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options  \n",
    "from selenium.webdriver.chrome.options import Options  \n",
    "\n",
    "\n",
    "# set up the driver and navigate to the page\n",
    "url = 'https://understat.com/team/Manchester_City/2018'\n",
    "driver = webdriver.Chrome(executable_path='/Users/lhe339/Desktop/chromedriver_mac_arm64/chromedriver')\n",
    "driver.get(url)\n",
    "wait = WebDriverWait(driver, 300)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'stats')))\n",
    "\n",
    "# parse the HTML content using BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find the table containing the player stats\n",
    "table = soup.find('table', {'id': 'team-data'})\n",
    "\n",
    "# loop through each row in the table and extract the relevant stats\n",
    "for row in table.tbody.find_all('tr'):\n",
    "    player_name = row.find('a', {'class': 'player_name'}).text\n",
    "    goals = row.find('td', {'data-stat': 'goals'}).text\n",
    "    assists = row.find('td', {'data-stat': 'assists'}).text\n",
    "    xG = row.find('td', {'data-stat': 'xG'}).text\n",
    "    xA = row.find('td', {'data-stat': 'xA'}).text\n",
    "    print(f\"Player: {player_name}, Goals: {goals}, Assists: {assists}, xG: {xG}, xA: {xA}\")\n",
    "    \n",
    "# close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e87073f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'tbody'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [96]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m top_players \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop-player-stats-summary-grid\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Extract the required data for each player\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m player \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtop_players\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtbody\u001b[49m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     18\u001b[0m     name \u001b[38;5;241m=\u001b[39m player\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer-link\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     19\u001b[0m     apps \u001b[38;5;241m=\u001b[39m player\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappearances\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tbody'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://1xbet.whoscored.com/Teams/37/Show/Germany-Bayern-Munich'\n",
    "\n",
    "# Request the webpage content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the section of the webpage that lists the top players\n",
    "top_players = soup.find('table', {'id': 'top-player-stats-summary-grid'})\n",
    "\n",
    "# Extract the required data for each player\n",
    "for player in top_players.tbody.find_all('tr'):\n",
    "    name = player.find('a', {'class': 'player-link'}).text\n",
    "    apps = player.find('td', {'class': 'appearances'}).text\n",
    "    mins = player.find('td', {'class': 'minsPlayed'}).text\n",
    "    goals = player.find('td', {'class': 'goals'}).text\n",
    "    assists = player.find('td', {'class': 'goalAssist'}).text\n",
    "    yel = player.find('td', {'class': 'yellowCard'}).text\n",
    "    red = player.find('td', {'class': 'redCard'}).text\n",
    "    spg = player.find('td', {'class': 'shotsPerGame'}).text\n",
    "    ps = player.find('td', {'class': 'passSuccess'}).text\n",
    "    aerials_won = player.find('td', {'class': 'aerialsWonPerGame'}).text\n",
    "    motm = player.find('td', {'class': 'manOfTheMatch'}).text\n",
    "    rating = player.find('td', {'class': 'rating'}).text\n",
    "\n",
    "    # Print the extracted data for each player\n",
    "    print(name, apps, mins, goals, assists, yel, red, spg, ps, aerials_won, motm, rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27180989",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data = pd.read_csv('player_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5e7c6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>League</th>\n",
       "      <th>Team</th>\n",
       "      <th>Player</th>\n",
       "      <th>Goals</th>\n",
       "      <th>Assists</th>\n",
       "      <th>xG</th>\n",
       "      <th>xA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [League, Team, Player, Goals, Assists, xG, xA]\n",
       "Index: []"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45b2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
